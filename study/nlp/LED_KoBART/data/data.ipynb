{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "import datetime\n",
    "nowdate = datetime.datetime.now()\n",
    "from tqdm import tqdm\n",
    "from IPython import get_ipython\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from Library.bart_to_long import *\n",
    "from Library.data_preprocess import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import PreTrainedTokenizerFast, LEDForConditionalGeneration, AutoModel\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW, TrainingArguments\n",
    "\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning import *\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "import pytorch_lightning.metrics.functional as FM\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger, MLFlowLogger\n",
    "\n",
    "import mlflow\n",
    "from pyngrok import ngrok\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_func(data, sample_pct):\n",
    "    np.random.seed(123)\n",
    "    N = len(data)\n",
    "    sample_n = int(len(data)*sample_pct) # integer    \n",
    "    sample = data.take(np.random.permutation(N)[:sample_n])\n",
    "    return sample\n",
    "\n",
    "# context 토큰 길이 \n",
    "def token_len(text):\n",
    "    return len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    tar_train = pd.read_csv(path, encoding='utf-8', index_col = False)\n",
    "    train_data, val_data = train_test_split(tar_train, test_size=0.3, shuffle=True, stratify=tar_train['Subject'], random_state=42)\n",
    "    return train_data.to_csv('/Users/yechansmacbook/RS/portfolio/study/nlp/LED_KoBART/data/new_train_data.csv', encoding='utf-8', index=False), val_data.to_csv('/Users/yechansmacbook/RS/portfolio/study/nlp/LED_KoBART/data/new_val_data.csv', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "def sample_set(path):\n",
    "    tar_train = pd.read_csv(path, encoding='utf-8', index_col = False)\n",
    "    sample_set = tar_train.groupby('Subject',group_keys=False).apply(sampling_func, sample_pct = 0.2)\n",
    "    sample_set.reset_index(inplace=True)\n",
    "    del sample_set['index']\n",
    "    sample_set['con_token_len'] = sample_set['Content'].apply(token_len)\n",
    "    tar_train = sample_set[sample_set['con_token_len']<=4096]\n",
    "    del tar_train['con_token_len']\n",
    "    train_data, val_data = train_test_split(tar_train, test_size=0.3, shuffle=True, stratify=tar_train['Subject'], random_state=42)\n",
    "    return train_data.to_csv('/Users/yechansmacbook/RS/portfolio/study/nlp/LED_KoBART/data/train_data_new.csv', encoding='utf-8', index=False), val_data.to_csv('/Users/yechansmacbook/RS/portfolio/study/nlp/LED_KoBART/data/val_data_new.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data('/Users/yechansmacbook/RS/portfolio/study/nlp/LED_KoBART/data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_set('/Users/yechansmacbook/RS/portfolio/study/nlp/LED_KoBART/data/new_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('LED')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd1887a16ee6af488f85746835fafdd7ac3c3024d59eac38a0e55014a800f57a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
